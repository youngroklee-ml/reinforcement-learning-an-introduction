[
["index.html", "R scripts for Sutton &amp; Barto’s book Reinforcement Learning: An Introduction (2nd Edition) Preface", " R scripts for Sutton &amp; Barto’s book Reinforcement Learning: An Introduction (2nd Edition) Youngrok Lee 2019-04-27 Preface This is R script that reproduces ShangtongZhang’s Python code for Sutton &amp; Barto’s book Reinforcement Learning: An Introduction (Sutton and Barto 2018) References "],
["intro.html", "Chapter 1 Introduction 1.1 Example: Tic-Tac_Toe", " Chapter 1 Introduction library(tidyverse) 1.1 Example: Tic-Tac_Toe Python code exists here with the following copyright statement. ####################################################################### # Copyright (C) # # 2016 - 2018 Shangtong Zhang(zhangshangtong.cpp@gmail.com) # # 2016 Jan Hakenberg(jan.hakenberg@gmail.com) # # 2016 Tian Jun(tianjun.cpp@gmail.com) # # 2016 Kenta Shimada(hyperkentakun@gmail.com) # # Permission given to modify the code as long as you keep this # # declaration at the top # ####################################################################### library(R6) Board size: n &lt;- 3 Define State class: State &lt;- R6Class(&quot;State&quot;, list( # the board is represented by an n * n array, # 1 represents a chessman of the player who moves first, # -1 represents a chessman of another player # 0 represents an empty position data = matrix(0L, nrow = n, ncol = n), winner = NA_integer_, hash_val = NA_integer_, end = NA_integer_, # compute the hash value for one state, it&#39;s unique hash = function(...) { if (is.na(self$hash_val)) { self$hash_val &lt;- 0L for (i in self$data) { if (i == -1L) i &lt;- 2L self$hash_val &lt;- self$hash_val * 3L + i } } invisible(self$hash_val) }, # check whether a player has won the game, or it&#39;s a tie is_end = function(...) { if (!is.na(self$end)) invisible(self$end) results &lt;- c( # check row rowSums(self$data), # check columns colSums(self$data), # check diagonals sum(diag(self$data)), sum(diag(self$data[, rev(seq_len(n))])) ) if (any(results == n)) { self$winner &lt;- 1L self$end &lt;- TRUE } else if (any(results == -n)) { self$winner &lt;- -1L self$end &lt;- TRUE } else if (all(self$data != 0)) { # whether it&#39;s a tie self$winner &lt;- 0L self$end &lt;- TRUE } else { # game is still going on self$end = FALSE } invisible(self$end) }, # @symbol: 1 or -1 # put chessman symbol in position (i, j) next_state = function (i, j, symbol) { new_state &lt;- State$new() new_state$data &lt;- self$data new_state$data[i, j] &lt;- symbol invisible(new_state) }, # print the board print_state = function(...) { for (i in seq_len(n)) { cat(&quot;-------------\\n&quot;) symbols &lt;- c(&quot;x&quot;, &quot;0&quot;, &quot;*&quot;)[self$data[i, ] + 2L] cat(str_c(&quot;| &quot;, str_c(symbols, collapse = &quot; | &quot;), &quot; |\\n&quot;)) } cat(&quot;-------------\\n&quot;) }, # print message print = function(...) { cat(&quot;State class object\\n&quot;) } )) State$new()$print_state() ## ------------- ## | 0 | 0 | 0 | ## ------------- ## | 0 | 0 | 0 | ## ------------- ## | 0 | 0 | 0 | ## ------------- Generate all possible State objects, and store them as objects in environment: get_all_states_impl &lt;- function (current_state, current_symbol, all_states) { for (i in seq_len(n)) { for (j in seq_len(n)) { if (current_state$data[i, j] == 0) { new_state &lt;- current_state$next_state(i, j, current_symbol) new_hash &lt;- new_state$hash() hash_str &lt;- as.character(new_hash) if (!hash_str %in% rlang::env_names(all_states)) { is_end &lt;- new_state$is_end() all_states[[hash_str]] &lt;- list( state = new_state, end = is_end ) if (!is_end) { get_all_states_impl(new_state, -current_symbol, all_states) } } } } } } get_all_states &lt;- function () { current_symbol &lt;- 1L current_state &lt;- State$new() all_states &lt;- new.env() hash_str &lt;- as.character(current_state$hash()) all_states[[hash_str]] &lt;- list( state = current_state, end = current_state$is_end() ) get_all_states_impl(current_state, current_symbol, all_states) invisible(all_states) } # all possible board configurations all_states &lt;- get_all_states() max_hash &lt;- max(as.integer(rlang::env_names(all_states))) Define Judger class: Judger &lt;- R6Class(&quot;Judger&quot;, list( # @player1: the player who will move first, its chessman will be 1 # @player2: another player with a chessman -1 i = 0L, players = vector(&quot;list&quot;, length = 2), player_symbols = rep(NA_integer_, 2), current_state = NULL, initialize = function (player1, player2) { self$players[[1]] &lt;- player1 self$players[[2]] &lt;- player2 self$player_symbols[1] &lt;- 1L self$player_symbols[2] &lt;- -1L walk2(self$players, self$player_symbols, ~ .x$set_symbol(.y)) self$current_state &lt;- State$new() }, reset = function () { walk(self$players, ~ .x$reset()) self$i &lt;- 0L }, alternate = function() { self$i &lt;- (self$i + 1L) %% 2L invisible(self$players[[2L - self$i]]) }, play = function (print_state = FALSE) { self$reset() current_state &lt;- State$new() walk(self$players, ~ .x$set_state(current_state)) while(TRUE) { player &lt;- self$alternate() # debug # cat(&quot;current hash: &quot;, as.character(current_state$hash()), &quot;\\n&quot;) if (print_state) { current_state$print_state() } i_j_symbol &lt;- player$act() # debug # cat(&quot;i: &quot;, i_j_symbol[1], &quot;j: &quot;, i_j_symbol[2], &quot;symbol: &quot;, i_j_symbol[3], &quot;\\n&quot;) next_state_hash &lt;- current_state$next_state( i_j_symbol[1], i_j_symbol[2], i_j_symbol[3] )$hash() %&gt;% as.character() # debug # cat(&quot;next hash: &quot;, next_state_hash, &quot;\\n&quot;) current_state &lt;- all_states[[next_state_hash]]$state is_end &lt;- all_states[[next_state_hash]]$end walk(self$players, ~ .x$set_state(current_state)) if (is_end) { # debug # cat(&quot;play end\\n&quot;) if (print_state) { current_state$print_state() } # invisible(current_state$winner) return(current_state$winner) } } } )) Define Player class that is AI player: # AI player Player &lt;- R6Class(&quot;Player&quot;, list( estimations = NULL, step_size = NA_real_, epsilon = NA_real_, states = NULL, greedy = NULL, symbol = NA_integer_, initialize = function (step_size = 0.1, epsilon = 0.1) { self$estimations &lt;- rep(0, max_hash) self$step_size &lt;- step_size self$epsilon &lt;- epsilon self$states &lt;- list() self$greedy &lt;- list() }, reset = function () { self$states &lt;- list() self$greedy &lt;- list() }, set_state = function (state) { self$states[[length(self$states) + 1]] &lt;- state self$greedy[[length(self$greedy) + 1]] &lt;- TRUE }, set_symbol = function (symbol) { self$symbol &lt;- symbol for (hash_val in rlang::env_names(all_states)) { state &lt;- all_states[[hash_val]]$state is_end &lt;- all_states[[hash_val]]$end hash_val_int &lt;- as.integer(hash_val) if (is_end) { if (state$winner == self$symbol) { self$estimations[hash_val_int] &lt;- 1.0 } else if (state$winner == 0) { # we need to distinguish between a tie and a lose self$estimations[hash_val_int] &lt;- 0.5 } else { self$estimations[hash_val_int] &lt;- 0 } } else { self$estimations[hash_val_int] &lt;- 0.5 } } }, # update value estimation backup = function ( ) { # for debug # cat(&quot;player trajectory\\n&quot;) # walk(self$states, ~.x$print_state()) self$states &lt;- map_int(self$states, ~ as.integer(.x$hash())) for (i in rev(seq_len(length(self$states) - 1))) { state &lt;- self$states[i] td_error &lt;- self$greedy[[i]] * ( self$estimations[self$states[i + 1]] - self$estimations[state]) self$estimations[state] &lt;- self$estimations[state] + self$step_size * td_error } }, # choose an action based on the state act = function (...) { state &lt;- self$states[[length(self$states)]] # state$print_state() next_states &lt;- list() next_positions &lt;- list() for (i in seq_len(n)) { for (j in seq_len(n)) { if (state$data[i, j] == 0L) { ind &lt;- length(next_positions) + 1 next_positions[[ind]] &lt;- c(i, j) next_states[[ind]] &lt;- state$next_state( i, j, self$symbol)$hash() } } } if (runif(1L) &lt; self$epsilon) { action &lt;- c( next_positions[[sample.int(length(next_positions), 1L)]], self$symbol ) self$greedy[[length(self$greedy)]] &lt;- FALSE invisible(action) } values &lt;- map2_dfr(next_states, next_positions, ~ tibble( value = self$estimations[as.integer(.x)], i = .y[1], j = .y[2] ) ) %&gt;% sample_n(size = n()) %&gt;% arrange(desc(value)) %&gt;% slice(1) action &lt;- c( c(values$i, values$j), self$symbol ) invisible(action) } )) Let us check whether a play by two AI players work correctly. set.seed(900) Judger$new(Player$new(), Player$new())$play(print_state = TRUE) ## ------------- ## | 0 | 0 | 0 | ## ------------- ## | 0 | 0 | 0 | ## ------------- ## | 0 | 0 | 0 | ## ------------- ## ------------- ## | 0 | * | 0 | ## ------------- ## | 0 | 0 | 0 | ## ------------- ## | 0 | 0 | 0 | ## ------------- ## ------------- ## | 0 | * | 0 | ## ------------- ## | 0 | 0 | 0 | ## ------------- ## | 0 | x | 0 | ## ------------- ## ------------- ## | * | * | 0 | ## ------------- ## | 0 | 0 | 0 | ## ------------- ## | 0 | x | 0 | ## ------------- ## ------------- ## | * | * | 0 | ## ------------- ## | 0 | 0 | x | ## ------------- ## | 0 | x | 0 | ## ------------- ## ------------- ## | * | * | * | ## ------------- ## | 0 | 0 | x | ## ------------- ## | 0 | x | 0 | ## ------------- ## [1] 1 Now, let us train players through multiple plays between two AI players. train &lt;- function(epochs, print_every_n = 500) { player1 &lt;- Player$new(epsilon=0.01) player2 &lt;- Player$new(epsilon=0.01) judger &lt;- Judger$new(player1, player2) player1_win = 0.0 player2_win = 0.0 player1_win_prev = 0.0 player2_win_prev = 0.0 for (i in seq_len(epochs)) { winner &lt;- judger$play(print_state = FALSE) if (winner == 1L) { player1_win = player1_win + 1 } else if (winner == -1L) { player2_win = player2_win + 1 } if (i %% print_every_n == 0) { cat(str_glue( &quot;Epoch {(i %/% print_every_n - 1) * print_every_n + 1} -- {i}, &quot;, &quot;player 1 winrate: {(player1_win - player1_win_prev) / print_every_n}, &quot;, &quot;player 2 winrate: {(player2_win - player2_win_prev) / print_every_n}&quot;)) cat(&quot;\\n&quot;) player1_win_prev &lt;- player1_win player2_win_prev &lt;- player2_win } player1$backup() player2$backup() judger$reset() } } set.seed(900) train(1e+3, print_every_n = 100) ## Epoch 1 -- 100, player 1 winrate: 0.66, player 2 winrate: 0.3 ## Epoch 101 -- 200, player 1 winrate: 0.4, player 2 winrate: 0.18 ## Epoch 201 -- 300, player 1 winrate: 0.37, player 2 winrate: 0.2 ## Epoch 301 -- 400, player 1 winrate: 0.25, player 2 winrate: 0.05 ## Epoch 401 -- 500, player 1 winrate: 0, player 2 winrate: 0 ## Epoch 501 -- 600, player 1 winrate: 0, player 2 winrate: 0 ## Epoch 601 -- 700, player 1 winrate: 0, player 2 winrate: 0 ## Epoch 701 -- 800, player 1 winrate: 0, player 2 winrate: 0 ## Epoch 801 -- 900, player 1 winrate: 0, player 2 winrate: 0 ## Epoch 901 -- 1000, player 1 winrate: 0, player 2 winrate: 0 It is observed that Player 1 who plays first won more at the beginning, but after two AI players learned from few hundreds plays, the results were mostly tie. "],
["multi-armed-bandit.html", "Chapter 2 Multi-armed Bandit 2.1 Example: The 10-armed Testbed", " Chapter 2 Multi-armed Bandit library(tidyverse) 2.1 Example: The 10-armed Testbed Python code exists here with the following copyright statement. ####################################################################### # Copyright (C) # # 2016-2018 Shangtong Zhang(zhangshangtong.cpp@gmail.com) # # 2016 Tian Jun(tianjun.cpp@gmail.com) # # 2016 Artem Oboturov(oboturov@gmail.com) # # 2016 Kenta Shimada(hyperkentakun@gmail.com) # # Permission given to modify the code as long as you keep this # # declaration at the top # ####################################################################### library(R6) Bandit &lt;- R6Class(&quot;Bandit&quot;, list( # @k_arm: # of arms # @epsilon: probability for exploration in epsilon-greedy algorithm # @initial: initial estimation for each action # @step_size: constant step size for updating estimations # @sample_averages: if True, use sample averages to update estimations instead of constant step size # @UCB_param: if not None, use UCB algorithm to select action # @gradient: if True, use gradient based bandit algorithm # @gradient_baseline: if True, use average reward as baseline for gradient based bandit algorithm k = NA_integer_, step_size = NA_real_, sample_averages = FALSE, indices = NULL, time = 0L, UCB_param = NULL, gradient = FALSE, gradient_baseline = FALSE, average_reward = 0, true_reward = 0, epsilon = NA_real_, initial = NA_real_, initialize = function ( k_arm = 10, epsilon = 0, initial = 0, step_size = 0.1, sample_averages = FALSE, UCB_param = NULL, gradient = FALSE, gradient_baseline = FALSE, true_reward = 0 ) { self$k &lt;- k_arm self$step_size &lt;- step_size self$sample_averages &lt;- sample_averages self$indices &lt;- seq_len(self$k) self$time &lt;- 0L self$UCB_param &lt;- UCB_param self$gradient &lt;- gradient self$gradient_baseline &lt;- gradient_baseline self$average_reward &lt;- 0 self$true_reward &lt;- true_reward self$epsilon &lt;- epsilon self$initial &lt;- initial } )) Bandit$set(&quot;public&quot;, &quot;q_true&quot;, NULL) Bandit$set(&quot;public&quot;, &quot;q_estimation&quot;, NULL) Bandit$set(&quot;public&quot;, &quot;action_count&quot;, NULL) Bandit$set(&quot;public&quot;, &quot;best_action&quot;, NULL) Bandit$set(&quot;public&quot;, &quot;action_prob&quot;, NULL) Bandit$set(&quot;public&quot;, &quot;reset&quot;, function() { # set time to 0 self$time &lt;- 0L # real reward for each action self$q_true &lt;- rnorm(self$k) + self$true_reward # estimation for each action self$q_estimation &lt;- rep(0, self$k) + self$initial # # of chosen times for each action self$action_count = rep(0L, self$k) # action probability for gradient methods self$action_prob &lt;- rep(1/self$k, self$k) self$best_action &lt;- which.max(self$q_true) }) maxima &lt;- function(x) { ind &lt;- which(x == max(x, na.rm = TRUE)) if (length(ind) &gt; 1) { ind &lt;- sample(ind, 1) } invisible(ind) } # get an action for this bandit Bandit$set(&quot;public&quot;, &quot;act&quot;, function() { if (runif(1) &lt; self$epsilon) { return(sample(self$indices, 1)) } if (!is.null(self$UCB_param)) { UCB_estimation &lt;- self$q_estimation + self$UCB_param * sqrt(log(self$time + 1) / (self$action_count + 1e-5)) return(maxima(UCB_estimation)) } if (self$gradient) { exp_est &lt;- exp(self$q_estimation) self$action_prob &lt;- exp_est / sum(exp_est) return(sample(self$indices, 1, prob = self$action_prob)) } return(maxima(self$q_estimation)) }) Bandit$set(&quot;public&quot;, &quot;step&quot;, function(action) { # generate the reward under N(real reward, 1) reward &lt;- rnorm(1, self$q_true[action], 1) self$time &lt;- self$time + 1 self$average_reward &lt;- (self$time - 1.0) / self$time * self$average_reward + reward / self$time self$action_count[action] &lt;- self$action_count[action] + 1 if (self$sample_averages) { # update estimation using sample averages self$q_estimation[action] &lt;- self$q_estimation[action] + 1 / self$action_count[action] * (reward - self$q_estimation[action]) } else if (self$gradient) { gradient &lt;- - self$action_prob gradient[action] &lt;- gradient[action] + 1 baseline &lt;- self$average_reward * self$gradient_baseline self$q_estimation &lt;- self$q_estimation + self$step_size * (reward - baseline) * gradient } else { # update estimation with constant step size self$q_estimation[action] &lt;- self$q_estimation[action] + self$step_size * (reward - self$q_estimation[action]) } invisible(reward) }) simulate_bandit &lt;- function(bandit, time) { bandit$reset() best_action &lt;- vector(&quot;logical&quot;, length = time) reward &lt;- vector(&quot;numeric&quot;, length = time) for(t in seq_len(time)) { action &lt;- bandit$act() reward[t] &lt;- bandit$step(action) if (action == bandit$best_action) { best_action[t] &lt;- TRUE } } res &lt;- tibble( t = seq_len(time), best_action = best_action, reward = reward ) return(res) } 2.1.1 Figure 2-2 set.seed(2000) epsilons &lt;- c(0.00, 0.01, 0.10) runs &lt;- 2000 times &lt;- 1000 bandits &lt;- map(rep_along(seq_len(runs * length(epsilons)), epsilons), ~ Bandit$new(epsilon = .x, sample_averages = TRUE)) pb &lt;- progress_estimated(length(bandits)) sim_results &lt;- map_dfr(bandits, function(x) { pb$tick()$print() simulate_bandit(x, times) %&gt;% mutate(epsilon = x$epsilon) }) sim_results %&gt;% mutate(epsilon = factor(epsilon, levels = epsilons)) %&gt;% group_by(epsilon, t) %&gt;% summarize(avg_reward = mean(reward), pct_optimal = mean(best_action)) %&gt;% ungroup() %&gt;% gather(key = &quot;key&quot;, value = &quot;value&quot;, avg_reward, pct_optimal) %&gt;% ggplot(aes(x = t, y = value)) + geom_line(aes(color = epsilon)) + facet_wrap(~ key, nrow = 2, scales = &quot;free_y&quot;) 2.1.2 Figure 2-3 set.seed(2000) epsilons &lt;- c(0.00, 0.10) initial &lt;- c(5, 0) runs &lt;- 2000 times &lt;- 1000 bandits &lt;- map2(rep_along(seq_len(runs * length(epsilons)), epsilons), rep_along(seq_len(runs * length(initial)), initial), ~ Bandit$new(epsilon = .x, initial = .y, step_size = 0.1)) pb &lt;- progress_estimated(length(bandits)) sim_results &lt;- map_dfr(bandits, function(x) { pb$tick()$print() simulate_bandit(x, times) %&gt;% mutate( epsilon = x$epsilon, initial = x$initial) }) sim_results %&gt;% mutate( config = if_else( epsilon == 0, &quot;Optimistic, greedy&quot;, &quot;Realistic, epsilon-greedy&quot;) ) %&gt;% group_by(config, t) %&gt;% summarize(avg_reward = mean(reward), pct_optimal = mean(best_action)) %&gt;% ungroup() %&gt;% ggplot(aes(x = t, y = pct_optimal)) + geom_line(aes(color = config)) 2.1.3 Figure 2-4 set.seed(2000) bandits &lt;- c( Bandit$new(epsilon = 0, UCB_param = 2, sample_averages = TRUE), Bandit$new(epsilon = 0.1, sample_averages = TRUE) ) runs &lt;- 2000 times &lt;- 1000 bandits &lt;- rep_along(seq_len(runs * length(bandits)), bandits) pb &lt;- progress_estimated(length(bandits)) sim_results &lt;- map_dfr(bandits, function(x) { pb$tick()$print() simulate_bandit(x, times) %&gt;% mutate( epsilon = x$epsilon, UCB_param = x$UCB_param) }) sim_results %&gt;% mutate( config = if_else( !is.na(UCB_param), &quot;UCB, c = 2&quot;, &quot;epsilon-greedy,epsilon = 0.1&quot;) ) %&gt;% group_by(config, t) %&gt;% summarize(avg_reward = mean(reward), pct_optimal = mean(best_action)) %&gt;% ungroup() %&gt;% ggplot(aes(x = t, y = avg_reward)) + geom_line(aes(color = config)) 2.1.4 Figure 2-5 set.seed(2000) bandits &lt;- c( Bandit$new(gradient = TRUE, step_size = 0.1, gradient_baseline = TRUE, true_reward = 4), Bandit$new(gradient = TRUE, step_size = 0.1, gradient_baseline = FALSE, true_reward = 4), Bandit$new(gradient = TRUE, step_size = 0.4, gradient_baseline = TRUE, true_reward = 4), Bandit$new(gradient = TRUE, step_size = 0.4, gradient_baseline = FALSE, true_reward = 4) ) runs &lt;- 2000 times &lt;- 1000 bandits &lt;- rep_along(seq_len(runs * length(bandits)), bandits) pb &lt;- progress_estimated(length(bandits)) sim_results &lt;- map_dfr(bandits, function(x) { pb$tick()$print() simulate_bandit(x, times) %&gt;% mutate( step_size = x$step_size, baseline = x$gradient_baseline) }) sim_results %&gt;% mutate( config = str_glue(&quot;Step Size: {step_size}, Baseline: {baseline}&quot;) ) %&gt;% group_by(config, t) %&gt;% summarize(avg_reward = mean(reward), pct_optimal = mean(best_action)) %&gt;% ungroup() %&gt;% ggplot(aes(x = t, y = pct_optimal)) + geom_line(aes(color = config)) 2.1.5 Figure 2-6 set.seed(2000) bandits &lt;- c( # epsilon-greedy w/ sample average Bandit$new(epsilon = 1/128, sample_averages = TRUE), Bandit$new(epsilon = 1/64, sample_averages = TRUE), Bandit$new(epsilon = 1/32, sample_averages = TRUE), Bandit$new(epsilon = 1/16, sample_averages = TRUE), Bandit$new(epsilon = 1/8, sample_averages = TRUE), Bandit$new(epsilon = 1/4, sample_averages = TRUE), # gradient bandit Bandit$new(gradient = TRUE, step_size = 1/32, gradient_baseline = TRUE), Bandit$new(gradient = TRUE, step_size = 1/16, gradient_baseline = TRUE), Bandit$new(gradient = TRUE, step_size = 1/8, gradient_baseline = TRUE), Bandit$new(gradient = TRUE, step_size = 1/4, gradient_baseline = TRUE), Bandit$new(gradient = TRUE, step_size = 1/2, gradient_baseline = TRUE), Bandit$new(gradient = TRUE, step_size = 1, gradient_baseline = TRUE), Bandit$new(gradient = TRUE, step_size = 2, gradient_baseline = TRUE), Bandit$new(gradient = TRUE, step_size = 4, gradient_baseline = TRUE), # UCB Bandit$new(epsilon = 0, UCB_param = 1/16, sample_averages = TRUE), Bandit$new(epsilon = 0, UCB_param = 1/8, sample_averages = TRUE), Bandit$new(epsilon = 0, UCB_param = 1/4, sample_averages = TRUE), Bandit$new(epsilon = 0, UCB_param = 1/2, sample_averages = TRUE), Bandit$new(epsilon = 0, UCB_param = 1, sample_averages = TRUE), Bandit$new(epsilon = 0, UCB_param = 2, sample_averages = TRUE), Bandit$new(epsilon = 0, UCB_param = 4, sample_averages = TRUE), # Optimistic greedy Bandit$new(epsilon = 0, initial = 1/4, step_size = 0.1), Bandit$new(epsilon = 0, initial = 1/2, step_size = 0.1), Bandit$new(epsilon = 0, initial = 1, step_size = 0.1), Bandit$new(epsilon = 0, initial = 2, step_size = 0.1), Bandit$new(epsilon = 0, initial = 4, step_size = 0.1) ) runs &lt;- 500 times &lt;- 1000 bandits &lt;- rep_along(seq_len(runs * length(bandits)), bandits) pb &lt;- progress_estimated(length(bandits)) sim_results &lt;- map_dfr(bandits, function(x) { pb$tick()$print() simulate_bandit(x, times) tibble( epsilon = x$epsilon, gradient = x$gradient, initial = x$initial, step_size = x$step_size, UCB_param = if_else(is.null(x$UCB_param), NA_real_, x$UCB_param), sample_averages = x$sample_averages, avg_reward = x$average_reward ) }) sim_summary &lt;- sim_results %&gt;% mutate( method = if_else( !is.na(UCB_param), &quot;UCB&quot;, if_else( gradient, &quot;gradient bandit&quot;, if_else( sample_averages, &quot;epsilon-greedy&quot;, &quot;optimistic-greedy&quot; ) ) ), param = if_else( !is.na(UCB_param), UCB_param, if_else( gradient, step_size, if_else( sample_averages, epsilon, initial ) ) ), ) %&gt;% group_by(method, param) %&gt;% summarize(avg_reward = mean(avg_reward)) sim_summary %&gt;% ggplot(aes(x = log2(param), y = avg_reward)) + geom_line(aes(color = method)) "]
]
