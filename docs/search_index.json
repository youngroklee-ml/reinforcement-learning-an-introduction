[
["index.html", "R scripts for Sutton &amp; Barto’s book Reinforcement Learning: An Introduction (2nd Edition) Preface", " R scripts for Sutton &amp; Barto’s book Reinforcement Learning: An Introduction (2nd Edition) Youngrok Lee 2019-04-14 Preface This is R script that reproduces ShangtongZhang’s Python code for Sutton &amp; Barto’s book Reinforcement Learning: An Introduction (Sutton and Barto 2018) References "],
["intro.html", "Chapter 1 Introduction 1.1 Example: Tic-Tac_Toe", " Chapter 1 Introduction library(tidyverse) 1.1 Example: Tic-Tac_Toe Python code exists here with the following copyright statement. ####################################################################### # Copyright (C) # # 2016 - 2018 Shangtong Zhang(zhangshangtong.cpp@gmail.com) # # 2016 Jan Hakenberg(jan.hakenberg@gmail.com) # # 2016 Tian Jun(tianjun.cpp@gmail.com) # # 2016 Kenta Shimada(hyperkentakun@gmail.com) # # Permission given to modify the code as long as you keep this # # declaration at the top # ####################################################################### library(R6) Board size: n &lt;- 3 Define State class: State &lt;- R6Class(&quot;State&quot;, list( # the board is represented by an n * n array, # 1 represents a chessman of the player who moves first, # -1 represents a chessman of another player # 0 represents an empty position data = matrix(0L, nrow = n, ncol = n), winner = NA_integer_, hash_val = NA_integer_, end = NA_integer_, # compute the hash value for one state, it&#39;s unique hash = function(...) { if (is.na(self$hash_val)) { self$hash_val &lt;- 0L for (i in self$data) { if (i == -1L) i &lt;- 2L self$hash_val &lt;- self$hash_val * 3L + i } } invisible(self$hash_val) }, # check whether a player has won the game, or it&#39;s a tie is_end = function(...) { if (!is.na(self$end)) invisible(self$end) results &lt;- c( # check row rowSums(self$data), # check columns colSums(self$data), # check diagonals sum(diag(self$data)), sum(diag(self$data[, rev(seq_len(n))])) ) if (any(results == n)) { self$winner &lt;- 1L self$end &lt;- TRUE } else if (any(results == -n)) { self$winner &lt;- -1L self$end &lt;- TRUE } else if (all(self$data != 0)) { # whether it&#39;s a tie self$winner &lt;- 0L self$end &lt;- TRUE } else { # game is still going on self$end = FALSE } invisible(self$end) }, # @symbol: 1 or -1 # put chessman symbol in position (i, j) next_state = function (i, j, symbol) { new_state &lt;- State$new() new_state$data &lt;- self$data new_state$data[i, j] &lt;- symbol invisible(new_state) }, # print the board print_state = function(...) { for (i in seq_len(n)) { cat(&quot;-------------\\n&quot;) symbols &lt;- c(&quot;x&quot;, &quot;0&quot;, &quot;*&quot;)[self$data[i, ] + 2L] cat(str_c(&quot;| &quot;, str_c(symbols, collapse = &quot; | &quot;), &quot; |\\n&quot;)) } cat(&quot;-------------\\n&quot;) }, # print message print = function(...) { cat(&quot;State class object\\n&quot;) } )) State$new()$print_state() ## ------------- ## | 0 | 0 | 0 | ## ------------- ## | 0 | 0 | 0 | ## ------------- ## | 0 | 0 | 0 | ## ------------- Generate all possible State objects, and store them as objects in environment: get_all_states_impl &lt;- function (current_state, current_symbol, all_states) { for (i in seq_len(n)) { for (j in seq_len(n)) { if (current_state$data[i, j] == 0) { new_state &lt;- current_state$next_state(i, j, current_symbol) new_hash &lt;- new_state$hash() hash_str &lt;- as.character(new_hash) if (!hash_str %in% rlang::env_names(all_states)) { is_end &lt;- new_state$is_end() all_states[[hash_str]] &lt;- list( state = new_state, end = is_end ) if (!is_end) { get_all_states_impl(new_state, -current_symbol, all_states) } } } } } } get_all_states &lt;- function () { current_symbol &lt;- 1L current_state &lt;- State$new() all_states &lt;- new.env() hash_str &lt;- as.character(current_state$hash()) all_states[[hash_str]] &lt;- list( state = current_state, end = current_state$is_end() ) get_all_states_impl(current_state, current_symbol, all_states) invisible(all_states) } # all possible board configurations all_states &lt;- get_all_states() max_hash &lt;- max(as.integer(rlang::env_names(all_states))) Define Judger class: Judger &lt;- R6Class(&quot;Judger&quot;, list( # @player1: the player who will move first, its chessman will be 1 # @player2: another player with a chessman -1 i = 0L, players = vector(&quot;list&quot;, length = 2), player_symbols = rep(NA_integer_, 2), current_state = NULL, initialize = function (player1, player2) { self$players[[1]] &lt;- player1 self$players[[2]] &lt;- player2 self$player_symbols[1] &lt;- 1L self$player_symbols[2] &lt;- -1L walk2(self$players, self$player_symbols, ~ .x$set_symbol(.y)) self$current_state &lt;- State$new() }, reset = function () { walk(self$players, ~ .x$reset()) self$i &lt;- 0L }, alternate = function() { self$i &lt;- (self$i + 1L) %% 2L invisible(self$players[[2L - self$i]]) }, play = function (print_state = FALSE) { self$reset() current_state &lt;- State$new() walk(self$players, ~ .x$set_state(current_state)) while(TRUE) { player &lt;- self$alternate() # debug # cat(&quot;current hash: &quot;, as.character(current_state$hash()), &quot;\\n&quot;) if (print_state) { current_state$print_state() } i_j_symbol &lt;- player$act() # debug # cat(&quot;i: &quot;, i_j_symbol[1], &quot;j: &quot;, i_j_symbol[2], &quot;symbol: &quot;, i_j_symbol[3], &quot;\\n&quot;) next_state_hash &lt;- current_state$next_state( i_j_symbol[1], i_j_symbol[2], i_j_symbol[3] )$hash() %&gt;% as.character() # debug # cat(&quot;next hash: &quot;, next_state_hash, &quot;\\n&quot;) current_state &lt;- all_states[[next_state_hash]]$state is_end &lt;- all_states[[next_state_hash]]$end walk(self$players, ~ .x$set_state(current_state)) if (is_end) { # debug # cat(&quot;play end\\n&quot;) if (print_state) { current_state$print_state() } # invisible(current_state$winner) return(current_state$winner) } } } )) Define Player class that is AI player: # AI player Player &lt;- R6Class(&quot;Player&quot;, list( estimations = NULL, step_size = NA_real_, epsilon = NA_real_, states = NULL, greedy = NULL, symbol = NA_integer_, initialize = function (step_size = 0.1, epsilon = 0.1) { self$estimations &lt;- rep(0, max_hash) self$step_size &lt;- step_size self$epsilon &lt;- epsilon self$states &lt;- list() self$greedy &lt;- list() }, reset = function () { self$states &lt;- list() self$greedy &lt;- list() }, set_state = function (state) { self$states[[length(self$states) + 1]] &lt;- state self$greedy[[length(self$greedy) + 1]] &lt;- TRUE }, set_symbol = function (symbol) { self$symbol &lt;- symbol for (hash_val in rlang::env_names(all_states)) { state &lt;- all_states[[hash_val]]$state is_end &lt;- all_states[[hash_val]]$end hash_val_int &lt;- as.integer(hash_val) if (is_end) { if (state$winner == self$symbol) { self$estimations[hash_val_int] &lt;- 1.0 } else if (state$winner == 0) { # we need to distinguish between a tie and a lose self$estimations[hash_val_int] &lt;- 0.5 } else { self$estimations[hash_val_int] &lt;- 0 } } else { self$estimations[hash_val_int] &lt;- 0.5 } } }, # update value estimation backup = function ( ) { # for debug # cat(&quot;player trajectory\\n&quot;) # walk(self$states, ~.x$print_state()) self$states &lt;- map_int(self$states, ~ as.integer(.x$hash())) for (i in rev(seq_len(length(self$states) - 1))) { state &lt;- self$states[i] td_error &lt;- self$greedy[[i]] * (self$estimations[self$states[i + 1]] - self$estimations[state]) self$estimations[state] &lt;- self$estimations[state] + self$step_size * td_error } }, # choose an action based on the state act = function (...) { state &lt;- self$states[[length(self$states)]] # state$print_state() next_states &lt;- list() next_positions &lt;- list() for (i in seq_len(n)) { for (j in seq_len(n)) { if (state$data[i, j] == 0L) { ind &lt;- length(next_positions) + 1 next_positions[[ind]] &lt;- c(i, j) next_states[[ind]] &lt;- state$next_state(i, j, self$symbol)$hash() } } } if (runif(1L) &lt; self$epsilon) { action &lt;- c( next_positions[[sample.int(length(next_positions), 1L)]], self$symbol ) self$greedy[[length(self$greedy)]] &lt;- FALSE invisible(action) } values &lt;- map2_dfr(next_states, next_positions, ~ tibble( value = self$estimations[as.integer(.x)], i = .y[1], j = .y[2] ) ) %&gt;% sample_n(size = n()) %&gt;% arrange(desc(value)) %&gt;% slice(1) action &lt;- c( c(values$i, values$j), self$symbol ) invisible(action) } )) Let us check whether a play by two AI players work correctly. set.seed(900) Judger$new(Player$new(), Player$new())$play(print_state = TRUE) ## ------------- ## | 0 | 0 | 0 | ## ------------- ## | 0 | 0 | 0 | ## ------------- ## | 0 | 0 | 0 | ## ------------- ## ------------- ## | 0 | * | 0 | ## ------------- ## | 0 | 0 | 0 | ## ------------- ## | 0 | 0 | 0 | ## ------------- ## ------------- ## | 0 | * | 0 | ## ------------- ## | 0 | 0 | 0 | ## ------------- ## | 0 | x | 0 | ## ------------- ## ------------- ## | * | * | 0 | ## ------------- ## | 0 | 0 | 0 | ## ------------- ## | 0 | x | 0 | ## ------------- ## ------------- ## | * | * | 0 | ## ------------- ## | 0 | 0 | x | ## ------------- ## | 0 | x | 0 | ## ------------- ## ------------- ## | * | * | * | ## ------------- ## | 0 | 0 | x | ## ------------- ## | 0 | x | 0 | ## ------------- ## [1] 1 Now, let us train players through multiple plays between two AI players. train &lt;- function(epochs, print_every_n = 500) { player1 &lt;- Player$new(epsilon=0.01) player2 &lt;- Player$new(epsilon=0.01) judger &lt;- Judger$new(player1, player2) player1_win = 0.0 player2_win = 0.0 player1_win_prev = 0.0 player2_win_prev = 0.0 for (i in seq_len(epochs)) { winner &lt;- judger$play(print_state = FALSE) if (winner == 1L) { player1_win = player1_win + 1 } else if (winner == -1L) { player2_win = player2_win + 1 } if (i %% print_every_n == 0) { cat(str_glue( &quot;Epoch {(i %/% print_every_n - 1) * print_every_n + 1} -- {i}, &quot;, &quot;player 1 winrate: {(player1_win - player1_win_prev) / print_every_n}, &quot;, &quot;player 2 winrate: {(player2_win - player2_win_prev) / print_every_n}&quot;)) cat(&quot;\\n&quot;) player1_win_prev &lt;- player1_win player2_win_prev &lt;- player2_win } player1$backup() player2$backup() judger$reset() } } set.seed(900) train(1e+3, print_every_n = 100) ## Epoch 1 -- 100, player 1 winrate: 0.66, player 2 winrate: 0.3 ## Epoch 101 -- 200, player 1 winrate: 0.4, player 2 winrate: 0.18 ## Epoch 201 -- 300, player 1 winrate: 0.37, player 2 winrate: 0.2 ## Epoch 301 -- 400, player 1 winrate: 0.25, player 2 winrate: 0.05 ## Epoch 401 -- 500, player 1 winrate: 0, player 2 winrate: 0 ## Epoch 501 -- 600, player 1 winrate: 0, player 2 winrate: 0 ## Epoch 601 -- 700, player 1 winrate: 0, player 2 winrate: 0 ## Epoch 701 -- 800, player 1 winrate: 0, player 2 winrate: 0 ## Epoch 801 -- 900, player 1 winrate: 0, player 2 winrate: 0 ## Epoch 901 -- 1000, player 1 winrate: 0, player 2 winrate: 0 It is observed that Player 1 who plays first won more at the beginning, but after two AI players learned from few hundreds plays, the results were mostly tie. "]
]
